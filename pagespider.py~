#! -*- coding:utf-8 -*-
import threading
import MySQLdb as db
from basespider import BaseSpider 
from bs4 import BeautifulSoup
from pybloom import BloomFilter
from imagespider import ImageSpider
from dboperation import DBOperation
from Queue import Queue

class PageSpider(BaseSpider):
	'extends from BaseSpider,scrpy the page'

    url_filter = BloomFilter(10000000)
    condition = threading.Condition()

	def __init__(self,start_url,thread_number,depth,queue):
            BaseSpider.__init__(self)
	    self.start_url = start_url
	    self.depth = depth
	    self.good_keywords = ['topit']
	    self.bad_keywords = ['login']
	    self.number = thread_number
	    self.queue = queue

	'递归，设置深度限制'
	def getPage(self,current_url,depth):
		#print 'current_url:',current_url
	    print 'thread:',self.number,'depth:',depth
	    if depth == self.depth:
	        return 
	    response = self.getResponse(current_url)
            if response == None:
 		return 
	    beautifulSoup = BeautifulSoup(response.text)
	    self.extractImages(beautifulSoup)
	    for next_url in self.extractUrls(beautifulSoup):
	        if next_url != None:
		    self.getPage(next_url,depth+1)

	'extract useful/fit self.depth urls'
	def extractUrls(self,soup):
	    counter = 0
	    a_tags = soup.find_all('a')
	        if a_tags != []:
	            for a_tag in a_tags:
			url = a_tag.get('href')
			flag = True
			if counter == self.depth:
			    yield None
			if url!=None and self.isFitUrl(url):
			    PageSpider.condition.acquire()
			    if not PageSpider.url_filter.add(url):
				PageSpider.condition.release()
				flag = False
				yield url
				counter += 1
			    if flag:
			    PageSpider.condition.release()

	def isFitUrl(self,url):
	    for keyword  in self.bad_keywords:
	        if keyword in url:
		    return False
	    for keyword in self.good_keywords:
	        if keyword in url:
		    return True
	    return False

	def extractImages(self,soup):
	    img_tags = soup.find_all('img')
	    if img_tags != []:
	        for img_tag in img_tags:
		    src = self.hasFitImage(img_tag)
		        PageSpider.condition.acquire()
			if src != None and not PageSpider.url_filter.add(src):		
			    print '************************current iamge'
			    print 'image url:',src
			    thread = ImageSpider(src,self.number,self.queue)
			    thread.start()
			    thread.join()
		        PageSpider.condition.release()				

	def hasFitImage(self,img):
	    width,height,src = self.getImgAttr(img)
	    for item in [width,height,src]:
		if item == None:
		    return None
	    width = width.replace('px','')
	    height = width.replace('px','')
	    if int(width) < 256 or int(height) < 256 or 'http' not in src:
	        return None
	    return src

	def getImgAttr(self,img):
	    return (img.get('width'),img.get('height'),img.get('src'))


	def run(self):
	    try:
	        self.getPage(self.start_url,0)
	    except Exception,e:
	        print 'Thread:',self.number,'error:',e

if __name__ == '__main__':
        print 'hello'
	queue = Queue()
	start_urls = ['http://www.topit.me/',
				  'http://www.topit.me/albums',
				  'http://www.topit.me/featured/albums',
				  'http://www.topit.me/art/albums',
				  'http://www.topit.me/photography/albums',
				  'http://www.topit.me/design/albums',
				  'http://www.topit.me/fashion/albums',
				  'http://www.topit.me/illustration/albums',
				  ]
	threads = []

	for i in range(len(start_urls)):
		threads.append(PageSpider(start_urls[i],i,len(start_urls),queue))
	
	thread = DBOperation(queue)
	thread.start()

	for i in range(len(threads)):
		threads[i].start()

	for i in range(len(threads)):
		threads[i].join()
	thread.join()











